{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index.core\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_parse import LlamaParse\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and producing embeddings of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw_texts/2-settext.pdf', 'raw_texts/3-settext.txt', 'raw_texts/3-slides.pdf', 'raw_texts/4-settext.pdf', 'raw_texts/4-slides.pdf', 'raw_texts/5-settext.pdf', 'raw_texts/9-settext.pdf']\n"
     ]
    }
   ],
   "source": [
    "PERSIST_DIR = \"./storage\"\n",
    "LIST_OF_DOCS = [\"raw_texts/\"+f for f in os.listdir(\"Raw_texts\")]\n",
    "\n",
    "print(LIST_OF_DOCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce index with SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = \"./storage_SimpleDirectoryReader\"\n",
    "parser = \"SDR\"\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"raw_texts\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0.0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\"\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0.0, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(OpenAI(model))\n",
    "\n",
    "def pingGPT(text_input):\n",
    "   response = query_engine.query(text_input)\n",
    "   return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JSON_quizzes_only_closed_questions/itembank-2.json', 'JSON_quizzes_only_closed_questions/itembank-3-4.json', 'JSON_quizzes_only_closed_questions/itembank-5-6.json', 'JSON_quizzes_only_closed_questions/itembank-9a.json', 'JSON_quizzes_only_closed_questions/itembank-9b.json']\n"
     ]
    }
   ],
   "source": [
    "# Open quizzes\n",
    "LIST_OF_QUIZZES = [\"JSON_quizzes_only_closed_questions/\"+f for f in os.listdir(\"./JSON_quizzes_only_closed_questions\") if f.endswith('.json')]\n",
    "print(LIST_OF_QUIZZES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the prompt for GPT to only respond with one of the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "new_prompt = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query. \"\n",
    "    \"Only answer with the letter and text of the answer which you think is correct.\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "new_tmpl = PromptTemplate(new_prompt)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": new_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass over the quizzes number 1\n",
      "\n",
      "Pass over the quizzes number 2\n",
      "\n",
      "Pass over the quizzes number 3\n",
      "\n",
      "Pass over the quizzes number 4\n",
      "\n",
      "Pass over the quizzes number 5\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each question and get GPT's response\n",
    "for pass_n in range(5):\n",
    "    print(f\"\\nPass over the quizzes number {pass_n+1}\")\n",
    "\n",
    "    total_nquestions = 0\n",
    "    total_ncorrects = 0\n",
    "    itembank_reports = []\n",
    "\n",
    "    for doc in LIST_OF_QUIZZES:\n",
    "        with open(doc, 'r', encoding='UTF-8') as file:\n",
    "            questions_JSON = json.load(file)\n",
    "        \n",
    "        itembank_number = re.search(r\"\\d[a|b]?(?:\\-\\d)?\", doc).group()\n",
    "        this_ncorrects = 0 # for correct answers in each separate itembank\n",
    "        this_nquestions = 0\n",
    "\n",
    "        #print(f\"----------processing itembank-{itembank_number}----------\")\n",
    "\n",
    "        for question in questions_JSON:\n",
    "            full_prompt = question['question'] + '\\n' + '\\n'.join(question['answers'])\n",
    "            given_answer = pingGPT(full_prompt)  # Get the response from GPT\n",
    "            question[\"GPT's response\"] = given_answer  # Append the response to the question\n",
    "            question[\"correct\"] = given_answer==question[\"correct answer(s)\"]\n",
    "\n",
    "            this_nquestions+=1\n",
    "            total_nquestions+=1\n",
    "            if given_answer==question[\"correct answer(s)\"]:\n",
    "                total_ncorrects+=1\n",
    "                this_ncorrects+=1\n",
    "            \n",
    "        itembank_reports.append(f\"itembank-{itembank_number}:\\t\\tCorrect answers {this_ncorrects} / total questions {this_nquestions} * 100 = {round(this_ncorrects/this_nquestions*100, 2)}%\")\n",
    "\n",
    "\n",
    "        # Save the updated JSON data to a new file\n",
    "        with open(f'./results/default_RAG_responses/default_RAG_{model}_responses-{itembank_number}.json', 'w') as outfile:\n",
    "            json.dump(questions_JSON, outfile, indent=2)\n",
    "\n",
    "    itembank_reports.append(f\"\\nOverall correct answers {total_ncorrects} / total questions {total_nquestions} * 100 = {round(total_ncorrects/total_nquestions*100, 2)}%\\n\")\n",
    "    itembank_reports.append(\"------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "    # save the prompt and number of correct questions\n",
    "    with open(f\"./results/default_rag_{model}_final_results.txt\", 'a', encoding='UTF-8') as report_file:\n",
    "        report_file.write('\\n'.join(itembank_reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe with Phoenix (not necessary to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\")\n",
    "\n",
    "with open('./JSON_questions_only_closed_questions/itembank-3-4.json', 'r', encoding='UTF-8') as file:\n",
    "    questions_JSON = json.load(file)\n",
    "\n",
    "for question in questions_JSON[18:19]:\n",
    "    full_prompt = question['question'] + '\\n' + '\\n'.join(question['answers'])\n",
    "    given_answer = pingGPT(full_prompt)  # Get the response from GPT\n",
    "print(given_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
