{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index.core\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_parse import LlamaParse\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from fuzzywuzzy import fuzz # for checking the levenshtein distance between retrieved chunks and oracle chunks\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "\n",
    "# variable for saving the report\n",
    "report = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open paths to docs and oracles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_DOCS = [\"./raw_texts/\"+f for f in os.listdir(\"./raw_texts\")]\n",
    "#print(LIST_OF_DOCS)\n",
    "\n",
    "LIST_OF_ORACLES = [\"./JSON_oracles/\"+f for f in os.listdir(\"./JSON_oracles/\") if f.endswith('.json')]\n",
    "#print(LIST_OF_ORACLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the parser and chunking method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SimpleDirectoryReader (SDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSIST_DIR = \"./storage_SimpleDirectoryReader\"\n",
    "parser_docs_type = \"SDR\"\n",
    "report.append(\"with SDR\")\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"raw_texts\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SDR manually-split docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# change the docs\n",
    "LIST_OF_DOCS = [\"./raw_texts_split_manually/\"+f for f in os.listdir(\"./raw_texts_split_manually/\")]\n",
    "print(LIST_OF_DOCS)\n",
    "print(f\"len(docs): {len(LIST_OF_DOCS)}\")\n",
    "\n",
    "report.append(\"with SDR on manually split docs\")\n",
    "parser_docs_type = \"manually_split_docs\"\n",
    "PERSIST_DIR = \"./storage_SimpleDirectoryReader_manually_split_docs\"\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"raw_texts_split_manually\").load_data()\n",
    "\n",
    "    # make index with a large chunk size so that none of the manually split documents gets split into multiple nodes\n",
    "    index = VectorStoreIndex.from_documents(documents, transformations=[SentenceSplitter(chunk_size=2048, chunk_overlap=0)])\n",
    "    \n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SDR with semantic node splitting with default embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "breakpoint_percentile_threshold = 70\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "PERSIST_DIR = f\"./storage_SDR_semantic_chunking_various_thresholds/storage_SDR_sem_{breakpoint_percentile_threshold}_text-embedding-ada-002\"\n",
    "parser_docs_type = \"on_semantic_chunking_ada-002\"\n",
    "report.append(\"with SDR + semantic doc splittlng + text-embedding-ada-002\")\n",
    "report.append(f\"breakpoint_percentile_threshold: {breakpoint_percentile_threshold}\")\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    documents = SimpleDirectoryReader(\"raw_texts\").load_data()\n",
    "    splitter = SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model, include_metadata=True)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    # load the documents and create the index\n",
    "    report.append(f\"len(nodes): {len(nodes)}\")\n",
    "    index = VectorStoreIndex(nodes)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SDR with semantic node splitting and a bigger embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "breakpoint_percentile_threshold = 70\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "\n",
    "PERSIST_DIR = f\"./storage_SDR_semantic_chunking_various_thresholds/storage_SDR_sem_{breakpoint_percentile_threshold}_text-embedding-3-large\"\n",
    "parser_docs_type = \"on_semantic_chunking_large_embedding_model\"\n",
    "report.append(\"with SDR + semantic doc splittlng + text-embedding-3-large\")\n",
    "report.append(f\"breakpoint_percentile_threshold: {breakpoint_percentile_threshold}\")\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    documents = SimpleDirectoryReader(\"raw_texts\").load_data()\n",
    "    splitter = SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model, include_metadata=True)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    # load the documents and create the index\n",
    "    report.append(f\"len(nodes): {len(nodes)}\")\n",
    "    index = VectorStoreIndex(nodes)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDR with semantic node splitting at optimal BP=70 with top_k=10, to test retrieval at various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "breakpoint_percentile_threshold = 70\n",
    "top_k = 10\n",
    "similarity_cutoff = 1\n",
    "#embedding_model = \"text-embedding-3-large\"\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "\n",
    "PERSIST_DIR = f\"./storage_SDR_semantic_chunking_various_thresholds/storage_SDR_sem_{breakpoint_percentile_threshold}_{embedding_model}\"\n",
    "parser_docs_type = \"on_semantic_chunking_large_embedding_model\"\n",
    "report.append(\"with SDR + semantic doc splittlng + text-embedding-3-large\")\n",
    "report.append(f\"breakpoint_percentile_threshold: {breakpoint_percentile_threshold}\")\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    documents = SimpleDirectoryReader(\"raw_texts\").load_data()\n",
    "    splitter = SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model, include_metadata=True)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    # load the documents and create the index\n",
    "    report.append(f\"len(nodes): {len(nodes)}\")\n",
    "    index = VectorStoreIndex(nodes)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity cutoff: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    similarity_cutoff\n",
    "except NameError:\n",
    "    similarity_cutoff = 0.7 # defaults siimilarity cutoff to 0.7 if not set\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)],\n",
    ")\n",
    "\n",
    "print(f\"similarity cutoff: {similarity_cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR basic:\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the retriever on self-created questions and count how many retrieved docs are retreieved correctly (based on the oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing oracle 2\n",
      "processing oracle 3\n",
      "processing oracle 4\n",
      "processing oracle 5\n",
      "processing oracle 9\n"
     ]
    }
   ],
   "source": [
    "# at least this percentage of the oracle chunk has to be in the retrieved node to be counted as correctly retrieved\n",
    "similarity_threshold = 70\n",
    "\n",
    "# save total number of questions and correct answers separately for my own quizzes and GPT-made\n",
    "total_nquestions = 0\n",
    "total_corr_chunks = 0\n",
    "\n",
    "document_node_titles = LIST_OF_ORACLES\n",
    "chunks_from_right_doc_total = []\n",
    "correctly_retrieved_chunks = []\n",
    "\n",
    "# Iterate over each question and get GPT's response\n",
    "for num_of_oracle, doc in enumerate(LIST_OF_ORACLES):\n",
    "    with open(doc, 'r', encoding='UTF-8') as file:\n",
    "        oracle_JSON = json.load(file)\n",
    "    \n",
    "    itembank_number = re.search(r\"\\d[a|b]?(?:\\-\\d)?\", doc).group()\n",
    "    print(f\"processing oracle {itembank_number}\")\n",
    "\n",
    "    this_corr_chunks = 0 # number of correctly retrieved chunks\n",
    "    this_nquestions = 0\n",
    "\n",
    "    chunks_from_right_doc_this = []\n",
    "    oracle_chunks_retrieved_this = []\n",
    "\n",
    "    for question in oracle_JSON:\n",
    "        \n",
    "        this_nquestions += 1\n",
    "        \n",
    "        full_prompt = question['question'] + '\\n' + '\\n'.join(question['answers'])\n",
    "        retrieved_nodes = retriever.retrieve(full_prompt)\n",
    "        \n",
    "        retrieved_titles = [n.node.metadata['file_name'] for n in retrieved_nodes]\n",
    "        retrieved_chunks = [n.node.text.replace(\"\\n\", \"\") for n in retrieved_nodes]\n",
    "\n",
    "        # make a list of lists with True/False for correctly retrived chunks from docs and right nodes\n",
    "        chunks_from_right_doc_this.append([re.search(r\"\\d\", title).group() == itembank_number for title in retrieved_titles])\n",
    "        oracle_chunks_retrieved_this.append([fuzz.partial_ratio(question['correct_text_chunk'], chunk) > similarity_threshold for chunk in retrieved_chunks])\n",
    "    \n",
    "    chunks_from_right_doc_total.append(chunks_from_right_doc_this)\n",
    "    correctly_retrieved_chunks.append(oracle_chunks_retrieved_this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting and displaying the overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall from correct doc as chunk number 1: 280/300\n",
      "Overall from correct doc as chunk number 2: 244/300\n",
      "Overall from correct doc as chunk number 3: 220/300\n",
      "Overall from correct doc as chunk number 4: 223/300\n",
      "Overall from correct doc as chunk number 5: 202/300\n",
      "Overall from correct doc as chunk number 6: 200/300\n",
      "Overall from correct doc as chunk number 7: 204/300\n",
      "Overall from correct doc as chunk number 8: 216/300\n",
      "Overall from correct doc as chunk number 9: 192/300\n",
      "Overall from correct doc as chunk number 10: 197/300\n",
      "Overall from correct chunk as chunk number 1: 161/300\n",
      "Overall from correct chunk as chunk number 2: 43/300\n",
      "Overall from correct chunk as chunk number 3: 39/300\n",
      "Overall from correct chunk as chunk number 4: 15/300\n",
      "Overall from correct chunk as chunk number 5: 7/300\n",
      "Overall from correct chunk as chunk number 6: 10/300\n",
      "Overall from correct chunk as chunk number 7: 5/300\n",
      "Overall from correct chunk as chunk number 8: 5/300\n",
      "Overall from correct chunk as chunk number 9: 6/300\n",
      "Overall from correct chunk as chunk number 10: 6/300\n"
     ]
    }
   ],
   "source": [
    "report.append(\"run number X\")\n",
    "report.append(f\"retriever: {retriever}\")\n",
    "report.append(f\"top_k = {top_k}\")\n",
    "report.append(f\"similarity cutoff: {similarity_threshold}\")\n",
    "\n",
    "for n, (correct_doc, correct_chunk) in enumerate(zip(chunks_from_right_doc_total, correctly_retrieved_chunks)):\n",
    "    \n",
    "    correct_doc_counts = [0 for n in range(top_k)]\n",
    "    correct_chunk_counts = [0 for n in range(top_k)]\n",
    "\n",
    "    # Iterate over each sublist in correct_chunks_total\n",
    "    for values in correct_doc:\n",
    "        # Check only the first three elements of each sublist\n",
    "        for i in range(top_k):\n",
    "            if values[i] == True:\n",
    "                correct_doc_counts[i] += 1\n",
    "\n",
    "\n",
    "    for values in correct_chunk:\n",
    "        # Check only the first three elements of each sublist\n",
    "        for i in range(top_k):\n",
    "            if values[i] == True:\n",
    "                correct_chunk_counts[i] += 1\n",
    "\n",
    "    #print(f\"\\nResults for {document_node_titles[n]}:\")\n",
    "    report.append(f\"\\nResults for {document_node_titles[n]}:\")\n",
    "    \n",
    "    for i, count in enumerate(correct_doc_counts):\n",
    "        #print(f\"from correct doc as chunk number {i+1}: {count}/{len(correct_doc)}\")\n",
    "        report.append(f\"from correct doc as chunk number {i+1}: {count}/{len(correct_doc)}\")\n",
    "    for i, count in enumerate(correct_chunk_counts):\n",
    "        #print(f\"from correct chunk as chunk number {i+1}: {count}/{len(correct_chunk)}\")\n",
    "        report.append(f\"from correct chunk as chunk number {i+1}: {count}/{len(correct_chunk)}\")\n",
    "    \n",
    "    # now handle the overall results\n",
    "    overall_correct_doc_counts = [0 for _ in range(top_k)]\n",
    "    overall_correct_chunk_counts = [0 for _ in range(top_k)]\n",
    "\n",
    "    # Accumulate total counts\n",
    "    for correct_doc, correct_chunk in zip(chunks_from_right_doc_total, correctly_retrieved_chunks):\n",
    "        for i in range(top_k):\n",
    "            overall_correct_doc_counts[i] += sum(values[i] for values in correct_doc)\n",
    "            overall_correct_chunk_counts[i] += sum(values[i] for values in correct_chunk)\n",
    "    # Append overall results to the report\n",
    "report.append(\"\\nOverall Results:\")\n",
    "for i, count in enumerate(overall_correct_doc_counts):\n",
    "    print(f\"Overall from correct doc as chunk number {i+1}: {count}/\" f\"{sum(len(sublist) for sublist in chunks_from_right_doc_total)}\")\n",
    "    report.append(f\"Overall from correct doc as chunk number {i+1}: {count}/\" f\"{sum(len(sublist) for sublist in chunks_from_right_doc_total)}\")\n",
    "for i, count in enumerate(overall_correct_chunk_counts):\n",
    "    print((f\"Overall from correct chunk as chunk number {i+1}: {count}/\" f\"{sum(len(sublist) for sublist in correctly_retrieved_chunks)}\"))\n",
    "    report.append(f\"Overall from correct chunk as chunk number {i+1}: {count}/\" f\"{sum(len(sublist) for sublist in correctly_retrieved_chunks)}\")\n",
    "\n",
    "report.append(\"\\n\\n--------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending the results to the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the report\n",
    "with open(f\"./results-comparison_with_oracle_reports_{parser_docs_type}.txt\", 'a') as report_file:\n",
    "    report_file.write('\\n'.join(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is the /k/ in \"dis[k]óvery\" unaspirated?\n",
      "a) Because it is at the end of the word\n",
      "b) Because it is positioned after a nasal sound\n",
      "c) Because it follows a fortis fricative\n",
      "d) Because it is followed by a vowel\n",
      "2-settext.pdf\n",
      "So \n",
      "while we can find aspirated plosives in words like [ph]eak and [kh]ool, the plosives \n",
      "in s[p]eak and s[k]ool are unaspirated , and so are the  ones in dis[k]óvery and \n",
      "fíf[t]éen (cf. níne[th]éen). \n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "2-settext.pdf\n",
      "So we could analyze the unaspirated voiceless \n",
      "plosives in s[p]eak, s[k]ool, dis[k]óvery, fíf[t]éen and káf[t]an phonologically as lenis \n",
      "plosives: s/b/eak, s/g/ool, dis/g/óvery, fíf/d/éen and káf/d/an. Actually, th e \n",
      "identification of such clusters as fortis fricative + lenis plosive clusters is reflected \n",
      "in the spelling of Wesh: ‘Spain’ is Sbaen and ‘school’ is ysgol. \n",
      "\n",
      "------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./JSON_oracles/oracle-itembank-2.json\", 'r', encoding='UTF-8') as file:\n",
    "    oracle_JSON = json.load(file)\n",
    "\n",
    "#retriever = vector_index.as_retriever()\n",
    "\n",
    "question = oracle_JSON[24]['question'] + '\\n' + '\\n'.join(oracle_JSON[24]['answers'])\n",
    "print(question)\n",
    "nodes = retriever.retrieve(question)\n",
    "#print(nodes[0])\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.node.metadata['file_name'])\n",
    "    #print(node.node.text.split('\\n')[0])\n",
    "    #print(document_node_titles[1])\n",
    "    #print(document_node_titles[0] in node.node.text.split('\\n')[1] )\n",
    "    print(node.node.text)\n",
    "    print(\"------------------\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
