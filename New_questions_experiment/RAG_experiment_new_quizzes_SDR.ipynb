{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index.core\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_parse import LlamaParse\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and producing embeddings of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw_texts/2-settext.pdf', 'raw_texts/3-settext.txt', 'raw_texts/4-settext.pdf', 'raw_texts/5-settext.pdf', 'raw_texts/9-settext.pdf']\n"
     ]
    }
   ],
   "source": [
    "PERSIST_DIR = \"./storage_SimpleDirectoryReader\"\n",
    "LIST_OF_DOCS = [\"raw_texts/\"+f for f in os.listdir(\"Raw_texts\")]\n",
    "\n",
    "print(LIST_OF_DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"raw_texts\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Setting up RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\"\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0.0, model=model)\n",
    "query_engine = index.as_query_engine(OpenAI(model=model))\n",
    "\n",
    "def pingGPT(text_input):\n",
    "   response = query_engine.query(text_input)\n",
    "   return response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the prompt for GPT to only respond with one of the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "new_prompt = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\"\n",
    "    \"Only answer with the letter and text of the answer which you think is correct.\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "new_tmpl = PromptTemplate(new_prompt)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": new_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JSON_quizzes/GPT-made_quizzes/itembank-2.json', 'JSON_quizzes/GPT-made_quizzes/itembank-3.json', 'JSON_quizzes/GPT-made_quizzes/itembank-4.json', 'JSON_quizzes/GPT-made_quizzes/itembank-5.json', 'JSON_quizzes/GPT-made_quizzes/itembank-9.json', 'JSON_quizzes/my_own_quizzes/itembank-2.json', 'JSON_quizzes/my_own_quizzes/itembank-3.json', 'JSON_quizzes/my_own_quizzes/itembank-4.json', 'JSON_quizzes/my_own_quizzes/itembank-5.json', 'JSON_quizzes/my_own_quizzes/itembank-9.json']\n",
      "number of quizzes: 10\n"
     ]
    }
   ],
   "source": [
    "GPT_made_quizzes = [\"JSON_quizzes/GPT-made_quizzes/\"+f for f in os.listdir(\"./JSON_quizzes/GPT-made_quizzes\") if f.endswith('.json')]\n",
    "my_own_quizzes = [\"JSON_quizzes/my_own_quizzes/\"+f for f in os.listdir(\"./JSON_quizzes/my_own_quizzes\") if f.endswith('.json')]\n",
    "LIST_OF_QUIZZES = GPT_made_quizzes + my_own_quizzes\n",
    "\n",
    "print(LIST_OF_QUIZZES)\n",
    "print(\"number of quizzes:\", len(LIST_OF_QUIZZES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------processing itembank-2-GPT-made_questions----------\n",
      "----------processing itembank-3-GPT-made_questions----------\n",
      "----------processing itembank-4-GPT-made_questions----------\n",
      "----------processing itembank-5-GPT-made_questions----------\n",
      "----------processing itembank-9-GPT-made_questions----------\n",
      "----------processing itembank-2-own_questions----------\n",
      "----------processing itembank-3-own_questions----------\n",
      "----------processing itembank-4-own_questions----------\n",
      "----------processing itembank-5-own_questions----------\n",
      "----------processing itembank-9-own_questions----------\n"
     ]
    }
   ],
   "source": [
    "# save total number of questions and correct answers separately for my own quizzes and GPT-made\n",
    "total_nquestions_own = 0\n",
    "total_ncorrects_own = 0\n",
    "total_nquestions_GPT = 0\n",
    "total_ncorrects_GPT = 0\n",
    "\n",
    "itembank_reports = [] # to write the itembank number and number of correct answers.\n",
    "\n",
    "# Iterate over each question and get GPT's response\n",
    "for doc in LIST_OF_QUIZZES:\n",
    "    with open(doc, 'r', encoding='UTF-8') as file:\n",
    "        itembank_JSON = json.load(file)\n",
    "    \n",
    "    itembank_number = re.search(r\"\\d[a|b]?(?:\\-\\d)?\", doc).group()\n",
    "    quiz_type = \"own_questions\" if re.search(r\"my_own_quizzes\", doc) else \"GPT-made_questions\" # returns None for GPT-made questions\n",
    "    this_ncorrects = 0 # for correct answers in each separate itembank\n",
    "    this_nquestions = 0\n",
    "\n",
    "    print(f\"----------processing itembank-{itembank_number}-{quiz_type}----------\")\n",
    "\n",
    "    for question in itembank_JSON:\n",
    "        full_prompt = question['question'] + '\\n' + '\\n'.join(question['answers'])\n",
    "        given_answer = pingGPT(full_prompt)  # Get the response from GPT\n",
    "        question[\"GPT's response\"] = given_answer  # Append the response to the question\n",
    "        question[\"correct\"] = given_answer==question[\"correct answer(s)\"]\n",
    "\n",
    "        if quiz_type == \"own_questions\":\n",
    "            if given_answer==question[\"correct answer(s)\"]: \n",
    "                total_ncorrects_own+=1\n",
    "                this_ncorrects+=1\n",
    "            total_nquestions_own+=1\n",
    "            this_nquestions+=1\n",
    "        else:\n",
    "            if given_answer==question[\"correct answer(s)\"]: \n",
    "                total_ncorrects_GPT+=1\n",
    "                this_ncorrects+=1\n",
    "            total_nquestions_GPT+=1\n",
    "            this_nquestions+=1\n",
    "        \n",
    "    itembank_reports.append(f\"itembank-{itembank_number}_{quiz_type}:\\t\\tCorrect answers {this_ncorrects} / total questions {this_nquestions} * 100 = {round(this_ncorrects/this_nquestions*100, 2)}%\")\n",
    "\n",
    "    # Save the updated JSON data to a new file\n",
    "    with open(f'./results/default_RAG_responses/with_updated_prompt_SimpleDirectoryReader/default_RAG_{model}_responses-SDR-{itembank_number}_{quiz_type}.json', 'w') as outfile:\n",
    "        json.dump(itembank_JSON, outfile, indent=2)\n",
    "\n",
    "itembank_reports.append(f\"\\nOverall correct answers for GPT-made {total_ncorrects_GPT} / total questions {total_nquestions_GPT} * 100 = {round(total_ncorrects_GPT/total_nquestions_GPT*100, 2)}%\")\n",
    "itembank_reports.append(f\"Overall correct answers for own {total_ncorrects_own} / total questions {total_nquestions_own} * 100 = {round(total_ncorrects_own/total_nquestions_own*100, 2)}%\\n\")\n",
    "itembank_reports.append(\"\\n------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# save the prompt and number of correct questions\n",
    "with open(f\"./results/default_rag_{model}_SimpleDirReader_results.txt\", 'a', encoding='UTF-8') as report_file:\n",
    "    report_file.write('\\n'.join(itembank_reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe with Phoenix (not necessary to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d) As long monophthongs\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\")\n",
    "\n",
    "with open('./JSON_quizzes/my_own_quizzes/itembank-3.json', 'r', encoding='UTF-8') as file:\n",
    "    questions_JSON = json.load(file)\n",
    "\n",
    "for question in questions_JSON[18:19]:\n",
    "    full_prompt = question['question'] + '\\n' + '\\n'.join(question['answers'])\n",
    "    given_answer = pingGPT(full_prompt)  # Get the response from GPT\n",
    "print(given_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
