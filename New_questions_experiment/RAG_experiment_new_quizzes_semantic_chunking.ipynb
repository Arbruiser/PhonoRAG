{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_index.core\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_parse import LlamaParse\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw_texts/2-settext.pdf', 'raw_texts/3-settext.txt', 'raw_texts/4-settext.pdf', 'raw_texts/5-settext.pdf', 'raw_texts/9-settext.pdf']\n"
     ]
    }
   ],
   "source": [
    "BP_threshold = 30\n",
    "top_k = 24 # set how many chunks are given as context to GPT\n",
    "\n",
    "PERSIST_DIR = f\"./experiments_with_oracle/storage_SDR_semantic_chunking_various_thresholds/storage_SDR_sem_{BP_threshold}_text-embedding-ada-002\"\n",
    "LIST_OF_DOCS = [\"raw_texts/\"+f for f in os.listdir(\"Raw_texts\")]\n",
    "\n",
    "print(LIST_OF_DOCS)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Setting up RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0.0, model=model)\n",
    "\n",
    "def pingGPT(text_input):\n",
    "   response = query_engine.query(text_input)\n",
    "   return response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JSON_quizzes/GPT-made_quizzes/itembank-2.json', 'JSON_quizzes/GPT-made_quizzes/itembank-3.json', 'JSON_quizzes/GPT-made_quizzes/itembank-4.json', 'JSON_quizzes/GPT-made_quizzes/itembank-5.json', 'JSON_quizzes/GPT-made_quizzes/itembank-9.json', 'JSON_quizzes/my_own_quizzes/itembank-2.json', 'JSON_quizzes/my_own_quizzes/itembank-3.json', 'JSON_quizzes/my_own_quizzes/itembank-4.json', 'JSON_quizzes/my_own_quizzes/itembank-5.json', 'JSON_quizzes/my_own_quizzes/itembank-9.json']\n",
      "number of quizzes: 10\n"
     ]
    }
   ],
   "source": [
    "GPT_made_quizzes = [\"JSON_quizzes/GPT-made_quizzes/\"+f for f in os.listdir(\"./JSON_quizzes/GPT-made_quizzes\") if f.endswith('.json')]\n",
    "my_own_quizzes = [\"JSON_quizzes/my_own_quizzes/\"+f for f in os.listdir(\"./JSON_quizzes/my_own_quizzes\") if f.endswith('.json')]\n",
    "LIST_OF_QUIZZES = GPT_made_quizzes + my_own_quizzes\n",
    "\n",
    "print(LIST_OF_QUIZZES)\n",
    "print(\"number of quizzes:\", len(LIST_OF_QUIZZES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The retriever with default prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=top_k,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------processing itembank-2-GPT-made_questions----------\n",
      "----------processing itembank-3-GPT-made_questions----------\n",
      "----------processing itembank-4-GPT-made_questions----------\n",
      "----------processing itembank-5-GPT-made_questions----------\n",
      "----------processing itembank-9-GPT-made_questions----------\n",
      "----------processing itembank-2-own_questions----------\n",
      "----------processing itembank-3-own_questions----------\n",
      "----------processing itembank-4-own_questions----------\n",
      "----------processing itembank-5-own_questions----------\n",
      "----------processing itembank-9-own_questions----------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# save total number of questions and correct answers separately for my own quizzes and GPT-made\n",
    "total_nquestions_own = 0\n",
    "total_ncorrects_own = 0\n",
    "total_nquestions_GPT = 0\n",
    "total_ncorrects_GPT = 0\n",
    "\n",
    "itembank_reports = [] # to write the itembank number and number of correct answers.\n",
    "itembank_reports.append(f\"breakpoint percentile: {BP_threshold}\")\n",
    "itembank_reports.append(f\"top_k: {top_k}\")\n",
    "\n",
    "# Iterate over each question and get GPT's response\n",
    "for doc in LIST_OF_QUIZZES:\n",
    "    with open(doc, 'r', encoding='UTF-8') as file:\n",
    "        itembank_JSON = json.load(file)\n",
    "    \n",
    "    itembank_number = re.search(r\"\\d[a|b]?(?:\\-\\d)?\", doc).group()\n",
    "    quiz_type = \"own_questions\" if re.search(r\"my_own_quizzes\", doc) else \"GPT-made_questions\" # returns None for GPT-made questions\n",
    "    this_ncorrects = 0 # for correct answers in each separate itembank\n",
    "    this_nquestions = 0\n",
    "\n",
    "    print(f\"----------processing itembank-{itembank_number}-{quiz_type}----------\")\n",
    "\n",
    "    for question in itembank_JSON:\n",
    "        full_prompt = question['question'] + '\\n' + '\\n'.join(question['answers'])\n",
    "        given_answer = pingGPT(full_prompt)  # Get the response from GPT\n",
    "        question[\"GPT's response\"] = given_answer  # Append the response to the question\n",
    "        question[\"correct\"] = given_answer==question[\"correct answer(s)\"]\n",
    "\n",
    "        if quiz_type == \"own_questions\":\n",
    "            if given_answer==question[\"correct answer(s)\"]: \n",
    "                total_ncorrects_own+=1\n",
    "                this_ncorrects+=1\n",
    "            total_nquestions_own+=1\n",
    "            this_nquestions+=1\n",
    "        else:\n",
    "            if given_answer==question[\"correct answer(s)\"]: \n",
    "                total_ncorrects_GPT+=1\n",
    "                this_ncorrects+=1\n",
    "            total_nquestions_GPT+=1\n",
    "            this_nquestions+=1\n",
    "        \n",
    "    itembank_reports.append(f\"itembank-{itembank_number}_{quiz_type}:\\t\\tCorrect answers {this_ncorrects} / total questions {this_nquestions} * 100 = {round(this_ncorrects/this_nquestions*100, 2)}%\")\n",
    "\n",
    "    # Save the updated JSON data to a new file\n",
    "    with open(f'./results/improved_RAG_responses/semantic_chunking/RAG_{model}_responses-semantic_{BP_threshold}-{itembank_number}_{quiz_type}.json', 'w') as outfile:\n",
    "        json.dump(itembank_JSON, outfile, indent=2)\n",
    "\n",
    "itembank_reports.append(f\"\\nOverall correct answers for GPT-made {total_ncorrects_GPT} / total questions {total_nquestions_GPT} * 100 = {round(total_ncorrects_GPT/total_nquestions_GPT*100, 2)}%\")\n",
    "itembank_reports.append(f\"Overall correct answers for own {total_ncorrects_own} / total questions {total_nquestions_own} * 100 = {round(total_ncorrects_own/total_nquestions_own*100, 2)}%\\n\")\n",
    "itembank_reports.append(\"\\n------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "# save the prompt and number of correct questions\n",
    "with open(f\"./results/default_rag_{model}_semantic_chunking_results.txt\", 'a', encoding='UTF-8') as report_file:\n",
    "    report_file.write('\\n'.join(itembank_reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe with Phoenix (not necessary to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\"\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\")\n",
    "\n",
    "with open('./JSON_quizzes/my_own_quizzes/itembank-4.json', 'r', encoding='UTF-8') as file:\n",
    "    questions_JSON = json.load(file)\n",
    "\n",
    "for question in questions_JSON[18:19]:\n",
    "    full_prompt = question['question'] + '\\n' + '\\n'.join(question['answers'])\n",
    "    given_answer = pingGPT(full_prompt)  # Get the response from GPT\n",
    "print(given_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
